{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Data Pipeline for Corpus Ingestion",
        "description": "Implement the data ingestion and preprocessing pipeline to transform the secured training corpus into model-ready format.",
        "details": "Create ETL scripts using Python to handle the following steps:\n1. Ingest raw text from the secured corpus\n2. Clean and normalize text (remove duplicates, fix encoding issues)\n3. Tokenize content using the tokenizer matching the selected base model\n4. Split data into appropriate formats for fine-tuning (Q&A pairs or continuous text)\n5. Implement data versioning using DVC or similar tool\n6. Create validation checks to ensure data quality\n7. Set up storage for intermediate processed data\n\nImplementation should use libraries like pandas for data manipulation, Hugging Face's tokenizers for processing, and include proper error handling. The pipeline should be modular to allow for future extensions (e.g., incorporating external knowledge bases).",
        "testStrategy": "1. Verify data integrity by comparing record counts between raw and processed data\n2. Run validation tests to ensure no data corruption during processing\n3. Check tokenization quality on sample texts\n4. Validate that the output format matches the requirements for model training\n5. Test the versioning system by creating multiple data versions\n6. Measure processing time and resource usage to establish performance baselines",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement JSONL Data Loader and Field Extraction",
            "description": "Create a data loader module to read the weolbu_posts.jsonl file and extract the body and parsed_content fields from each record for processing.",
            "dependencies": [],
            "details": "Implement a Python class using pandas or ijson for efficient JSONL reading. Create functions to extract 'body' and 'parsed_content' fields, handle missing fields gracefully, and implement basic validation to ensure data integrity. Include error handling for malformed JSON records and logging for processing statistics.",
            "status": "done",
            "testStrategy": "Unit tests for JSONL parsing, field extraction accuracy, and error handling with malformed data samples"
          },
          {
            "id": 2,
            "title": "Build Text Preprocessing and Cleaning Pipeline",
            "description": "Develop comprehensive text preprocessing functions to clean and normalize the extracted body and parsed_content text data.",
            "dependencies": [
              1
            ],
            "details": "Implement functions using BeautifulSoup for HTML tag removal, regex patterns for special character normalization, whitespace cleanup, and encoding issue fixes. Create duplicate detection logic using text hashing. Include configurable preprocessing options and maintain original-to-processed text mapping for debugging.",
            "status": "done",
            "testStrategy": "Test cases for HTML removal, encoding fixes, duplicate detection, and preprocessing consistency across different text formats"
          },
          {
            "id": 3,
            "title": "Integrate Tokenization with Base Model Compatibility",
            "description": "Implement tokenization pipeline using Hugging Face tokenizers that matches the selected base model's tokenization scheme.",
            "dependencies": [
              2
            ],
            "details": "Use transformers library to load the appropriate tokenizer for the base model. Implement batch tokenization with proper handling of maximum sequence lengths, padding, and truncation. Create functions to convert tokenized data back to text for validation. Include token count statistics and vocabulary coverage analysis.",
            "status": "done",
            "testStrategy": "Validate tokenization consistency, test sequence length handling, and verify token-to-text reconstruction accuracy"
          },
          {
            "id": 4,
            "title": "Create Training Data Format Conversion and Splitting",
            "description": "Transform the preprocessed and tokenized data into appropriate training formats and implement data splitting strategies for fine-tuning.",
            "dependencies": [
              3
            ],
            "details": "Implement data formatters for different training approaches (continuous text for causal LM, Q&A pairs if applicable). Create train/validation/test splits with configurable ratios. Build HuggingFace Dataset objects with proper features and metadata. Include data balancing and sampling strategies to ensure representative training data.",
            "status": "done",
            "testStrategy": "Verify data format correctness, test split ratios, and validate HuggingFace Dataset compatibility with training frameworks"
          },
          {
            "id": 5,
            "title": "Implement Data Versioning and Quality Validation System",
            "description": "Set up data versioning using DVC and implement comprehensive quality validation checks for the processed training data.",
            "dependencies": [
              4
            ],
            "details": "Configure DVC for data versioning with remote storage setup. Implement quality validation checks including data distribution analysis, token count statistics, vocabulary coverage, and data integrity verification. Create automated pipeline orchestration with configurable parameters and comprehensive logging. Build data quality reports and metrics dashboard.",
            "status": "done",
            "testStrategy": "Test DVC versioning functionality, validate quality check accuracy, and verify pipeline reproducibility across different runs"
          }
        ]
      },
      {
        "id": 2,
        "title": "Select and Configure Base Model",
        "description": "Configure the selected base model LGAI-EXAONE/EXAONE-Deep-2.4B for downstream fine-tuning.",
        "details": "Base model has been selected: **LGAI-EXAONE/EXAONE-Deep-2.4B**.\n\nSteps to configure:\n1. Load the model and tokenizer from HuggingFace with `use_fast=True` and trust_remote_code.\n2. Use FP16/torch.bfloat16 to fit within the 48 GB VRAM budget.\n3. Enable gradient checkpointing and accumulation where necessary.\n4. Prepare LoRA adapters for parameter-efficient fine-tuning.\n5. Document rationale for choosing EXAONE-Deep-2.4B and provide memory/inference benchmarks.",
        "testStrategy": "1. Verify model loads successfully with FP16 precision\n2. Measure memory usage to confirm it stays within the 48GB limit\n3. Run basic inference tests to ensure the model produces coherent outputs\n4. Benchmark inference speed\n5. Test compatibility with the Hugging Face Trainer API\n6. Validate that optimization techniques (LoRA, etc.) are properly implemented",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement Model Training Pipeline",
        "description": "Develop the training infrastructure to fine-tune the selected base model on the preprocessed corpus using Hugging Face's Trainer API.",
        "details": "1. Set up PyTorch training environment with Hugging Face's Trainer API\n2. Configure training hyperparameters (learning rate, batch size, epochs)\n3. Implement gradient accumulation to handle memory constraints\n4. Set up checkpointing to save model states during training\n5. Integrate MLflow or similar tool for experiment tracking\n6. Create evaluation metrics (perplexity, accuracy on validation set)\n7. Implement early stopping based on validation metrics\n8. Configure model export in safe-tensors format\n\nThe training script should include:\n```python\nfrom transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport mlflow\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"model_name\")\nmodel = AutoModelForCausalLM.from_pretrained(\"model_name\", torch_dtype=torch.float16)\n\n# Configure training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch size of 16\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    fp16=True,  # Use FP16 precision\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    eval_steps=500,\n    save_steps=1000,\n    evaluation_strategy=\"steps\",\n    save_total_limit=3,\n    load_best_model_at_end=True,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Start training\ntrainer.train()\n\n# Save final model\ntrainer.save_model(\"./final_model\")\nmodel.save_pretrained(\"./final_model\", safe_serialization=True)  # Use safe-tensors format\ntokenizer.save_pretrained(\"./final_model\")\n```",
        "testStrategy": "1. Run a small-scale training job on a subset of data to validate the pipeline\n2. Monitor GPU memory usage during training to ensure it stays within limits\n3. Verify checkpoints are saved correctly and can be loaded\n4. Validate that metrics are properly logged in MLflow\n5. Test the early stopping mechanism\n6. Ensure the final model is correctly exported in safe-tensors format\n7. Compare training and validation loss curves to detect overfitting\n8. Evaluate the trained model on a held-out test set",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Build Inference API Service",
        "description": "Develop a RESTful or gRPC API service to serve the fine-tuned model for inference, handling incoming requests and returning model responses.",
        "details": "1. Create a containerized API service using FastAPI or similar framework\n2. Implement model loading with optimizations for inference (8-bit quantization if needed)\n3. Design API endpoints for text generation\n4. Add authentication and rate limiting\n5. Implement request/response logging\n6. Set up proper error handling and fallback responses\n7. Configure Docker for deployment\n\nExample FastAPI implementation:\n```python\nfrom fastapi import FastAPI, Depends, HTTPException, Security\nfrom fastapi.security import APIKeyHeader\nfrom pydantic import BaseModel\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport logging\n\napp = FastAPI()\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\nmodel = AutoModelForCausalLM.from_pretrained(\"./final_model\", torch_dtype=torch.float16)\nmodel.eval()\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# API key authentication\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\ndef get_api_key(api_key: str = Security(api_key_header)):\n    if api_key != \"your-api-key\":  # Replace with secure key validation\n        raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n    return api_key\n\nclass QueryRequest(BaseModel):\n    text: str\n    max_length: int = 512\n    temperature: float = 0.7\n\nclass QueryResponse(BaseModel):\n    response: str\n    request_id: str\n    timestamp: str\n\n@app.post(\"/generate\", response_model=QueryResponse)\nasync def generate_text(request: QueryRequest, api_key: str = Depends(get_api_key)):\n    try:\n        # Log request (metadata only)\n        request_id = str(uuid.uuid4())\n        logger.info(f\"Request {request_id}: length={len(request.text)}\")\n        \n        # Generate response\n        inputs = tokenizer(request.text, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs[\"input_ids\"].to(model.device),\n                max_length=request.max_length,\n                temperature=request.temperature,\n                do_sample=True\n            )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Log response (metadata only)\n        logger.info(f\"Response {request_id}: length={len(response)}\")\n        \n        return QueryResponse(\n            response=response,\n            request_id=request_id,\n            timestamp=datetime.now().isoformat()\n        )\n    except Exception as e:\n        logger.error(f\"Error processing request: {str(e)}\")\n        raise HTTPException(status_code=500, detail=\"Model inference failed\")\n```\n\nDockerfile:\n```dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```",
        "testStrategy": "1. Test API endpoints with sample queries\n2. Verify authentication and rate limiting work correctly\n3. Measure response times under various load conditions\n4. Test error handling by sending malformed requests\n5. Verify logging captures necessary information\n6. Test Docker container deployment\n7. Perform load testing to determine maximum throughput\n8. Validate memory usage during inference",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Vector Database for Retrieval",
        "description": "Build a vector embedding store of the corpus to enable retrieval-augmented generation for improved answer accuracy.",
        "details": "1. Select an appropriate vector database (FAISS, Pinecone, etc.)\n2. Create embeddings for all documents in the corpus using a suitable embedding model\n3. Implement efficient storage and indexing of the embeddings\n4. Develop a retrieval function to fetch relevant passages based on query similarity\n5. Set up a mechanism to update the vector store when new documents are added\n\nImplementation example:\n```python\nimport faiss\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\nclass VectorStore:\n    def __init__(self, embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n        self.model = AutoModel.from_pretrained(embedding_model_name)\n        self.model.eval()\n        self.document_store = []  # Store original documents\n        self.index = None\n        \n    def _get_embedding(self, text):\n        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        # Use mean pooling to get document embeddings\n        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n        return embeddings\n    \n    def add_documents(self, documents):\n        # Store original documents\n        start_idx = len(self.document_store)\n        self.document_store.extend(documents)\n        \n        # Create embeddings\n        embeddings = []\n        for doc in documents:\n            embedding = self._get_embedding(doc)\n            embeddings.append(embedding)\n        \n        embeddings = np.vstack(embeddings)\n        \n        # Create or update FAISS index\n        if self.index is None:\n            dimension = embeddings.shape[1]\n            self.index = faiss.IndexFlatL2(dimension)\n        \n        self.index.add(embeddings)\n        return start_idx, start_idx + len(documents) - 1\n    \n    def search(self, query, k=5):\n        query_embedding = self._get_embedding(query)\n        distances, indices = self.index.search(query_embedding, k)\n        results = []\n        for i, idx in enumerate(indices[0]):\n            if idx < len(self.document_store):\n                results.append({\n                    \"document\": self.document_store[idx],\n                    \"score\": float(distances[0][i])\n                })\n        return results\n    \n    def save(self, path):\n        faiss.write_index(self.index, f\"{path}/vector_index\")\n        with open(f\"{path}/documents.json\", \"w\") as f:\n            json.dump(self.document_store, f)\n    \n    def load(self, path):\n        self.index = faiss.read_index(f\"{path}/vector_index\")\n        with open(f\"{path}/documents.json\", \"r\") as f:\n            self.document_store = json.load(f)\n```",
        "testStrategy": "1. Test embedding generation with sample documents\n2. Verify vector storage and retrieval accuracy\n3. Measure query latency for different corpus sizes\n4. Test the system with edge cases (very short/long documents)\n5. Validate that document updates are correctly reflected in the index\n6. Compare retrieval results against expected relevant documents\n7. Benchmark memory usage for different corpus sizes\n8. Test save/load functionality for persistence",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Integrate Retrieval-Augmented Generation (RAG)",
        "description": "Enhance the inference API to incorporate retrieved documents as context for the model, improving factual accuracy and reducing hallucinations.",
        "details": "1. Extend the inference API to use the vector store for document retrieval\n2. Implement prompt engineering to incorporate retrieved passages into the context\n3. Develop a ranking mechanism to select the most relevant passages\n4. Add citation tracking to reference source documents\n5. Implement fallback mechanisms when no relevant documents are found\n\nImplementation example:\n```python\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom pydantic import BaseModel\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\napp = FastAPI()\n\n# Load model, tokenizer, and vector store\ntokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\nmodel = AutoModelForCausalLM.from_pretrained(\"./final_model\", torch_dtype=torch.float16)\nmodel.eval()\n\n# Initialize vector store\nvector_store = VectorStore()\nvector_store.load(\"./vector_store\")\n\nclass RAGRequest(BaseModel):\n    query: str\n    max_length: int = 512\n    temperature: float = 0.7\n    num_documents: int = 3\n\nclass RAGResponse(BaseModel):\n    response: str\n    sources: list\n    request_id: str\n\n@app.post(\"/rag_generate\", response_model=RAGResponse)\nasync def rag_generate(request: RAGRequest):\n    # Retrieve relevant documents\n    retrieved_docs = vector_store.search(request.query, k=request.num_documents)\n    \n    # Format prompt with retrieved context\n    context = \"\\n\\n\".join([doc[\"document\"] for doc in retrieved_docs])\n    prompt = f\"Context information:\\n{context}\\n\\nQuestion: {request.query}\\n\\nAnswer:\"\n    \n    # Generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs[\"input_ids\"].to(model.device),\n            max_length=request.max_length,\n            temperature=request.temperature,\n            do_sample=True\n        )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract just the answer part (after \"Answer:\")\n    answer = response.split(\"Answer:\")[-1].strip()\n    \n    # Return response with sources\n    sources = [{\n        \"text\": doc[\"document\"][:100] + \"...\",  # Truncate for display\n        \"score\": doc[\"score\"]\n    } for doc in retrieved_docs]\n    \n    return RAGResponse(\n        response=answer,\n        sources=sources,\n        request_id=str(uuid.uuid4())\n    )\n```",
        "testStrategy": "1. Compare answer quality with and without RAG\n2. Test with queries that require specific factual information\n3. Verify citation accuracy by checking if answers contain information from the retrieved documents\n4. Measure the impact on response time when adding retrieval\n5. Test edge cases where no relevant documents are found\n6. Evaluate the system on a test set of domain-specific questions\n7. Check for hallucinations by comparing answers to source documents\n8. Test with varying numbers of retrieved documents to find optimal settings",
        "priority": "medium",
        "dependencies": [
          4,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Develop User Interface",
        "description": "Create a clean, text-focused web interface for users to interact with the SLM, following the UX principles outlined in the PRD.",
        "details": "1. Design a responsive web interface using modern web frameworks (React, Vue, etc.)\n2. Implement a chat-like interface for question-answer interactions\n3. Add conversation history to maintain context\n4. Include UI elements for feedback collection (upvote/downvote)\n5. Design error states and loading indicators\n6. Ensure accessibility compliance\n7. Apply the organization's design language (colors, fonts)\n\nExample React component structure:\n```jsx\n// Main Chat Component\nimport React, { useState, useEffect, useRef } from 'react';\nimport './Chat.css';\n\nconst Chat = () => {\n  const [messages, setMessages] = useState([]);\n  const [input, setInput] = useState('');\n  const [loading, setLoading] = useState(false);\n  const messagesEndRef = useRef(null);\n\n  // Auto-scroll to bottom of messages\n  const scrollToBottom = () => {\n    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n  };\n\n  useEffect(() => {\n    scrollToBottom();\n  }, [messages]);\n\n  const handleSubmit = async (e) => {\n    e.preventDefault();\n    if (!input.trim()) return;\n    \n    // Add user message to chat\n    const userMessage = { text: input, sender: 'user', id: Date.now() };\n    setMessages(prev => [...prev, userMessage]);\n    setInput('');\n    setLoading(true);\n    \n    try {\n      // Call API\n      const response = await fetch('/api/generate', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ \n          text: input,\n          // Include conversation history for context\n          history: messages.map(m => `${m.sender === 'user' ? 'User' : 'Assistant'}: ${m.text}`).join('\\n')\n        })\n      });\n      \n      const data = await response.json();\n      \n      // Add bot response to chat\n      const botMessage = { \n        text: data.response, \n        sender: 'bot', \n        id: Date.now() + 1,\n        sources: data.sources || [] \n      };\n      setMessages(prev => [...prev, botMessage]);\n    } catch (error) {\n      // Handle error\n      const errorMessage = { \n        text: 'Sorry, I encountered an error. Please try again.', \n        sender: 'bot', \n        id: Date.now() + 1,\n        isError: true \n      };\n      setMessages(prev => [...prev, errorMessage]);\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  const handleFeedback = (messageId, isPositive) => {\n    // Send feedback to API\n    fetch('/api/feedback', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ messageId, isPositive })\n    });\n    \n    // Update UI to show feedback was received\n    setMessages(prev => prev.map(msg => \n      msg.id === messageId ? {...msg, feedback: isPositive ? 'positive' : 'negative'} : msg\n    ));\n  };\n\n  return (\n    <div className=\"chat-container\">\n      <div className=\"messages-container\">\n        {messages.length === 0 && (\n          <div className=\"empty-state\">\n            <h2>Ask me anything about our domain!</h2>\n            <p>I'm trained on our organization's knowledge base.</p>\n          </div>\n        )}\n        \n        {messages.map(message => (\n          <div \n            key={message.id} \n            className={`message ${message.sender} ${message.isError ? 'error' : ''}`}\n          >\n            <div className=\"message-text\">{message.text}</div>\n            \n            {message.sources && message.sources.length > 0 && (\n              <div className=\"sources\">\n                <details>\n                  <summary>Sources ({message.sources.length})</summary>\n                  <ul>\n                    {message.sources.map((source, i) => (\n                      <li key={i}>{source.text}</li>\n                    ))}\n                  </ul>\n                </details>\n              </div>\n            )}\n            \n            {message.sender === 'bot' && !message.isError && (\n              <div className=\"feedback-buttons\">\n                <button \n                  onClick={() => handleFeedback(message.id, true)}\n                  className={message.feedback === 'positive' ? 'active' : ''}\n                >\n                  üëç\n                </button>\n                <button \n                  onClick={() => handleFeedback(message.id, false)}\n                  className={message.feedback === 'negative' ? 'active' : ''}\n                >\n                  üëé\n                </button>\n              </div>\n            )}\n          </div>\n        ))}\n        \n        {loading && (\n          <div className=\"message bot loading\">\n            <div className=\"typing-indicator\">\n              <span></span>\n              <span></span>\n              <span></span>\n            </div>\n          </div>\n        )}\n        \n        <div ref={messagesEndRef} />\n      </div>\n      \n      <form onSubmit={handleSubmit} className=\"input-form\">\n        <input\n          type=\"text\"\n          value={input}\n          onChange={(e) => setInput(e.target.value)}\n          placeholder=\"Type your question here...\"\n          disabled={loading}\n        />\n        <button type=\"submit\" disabled={loading || !input.trim()}>\n          Send\n        </button>\n      </form>\n    </div>\n  );\n};\n\nexport default Chat;\n```",
        "testStrategy": "1. Test UI responsiveness across different devices and screen sizes\n2. Verify that conversation history is maintained correctly\n3. Test error handling and loading states\n4. Validate accessibility using automated tools (e.g., Lighthouse)\n5. Conduct usability testing with representative users\n6. Test feedback collection functionality\n7. Verify that the UI correctly displays sources when available\n8. Test performance with long conversations",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Feedback Collection and Analytics",
        "description": "Build the backend infrastructure to collect user feedback on model responses and track usage metrics for continuous improvement.",
        "details": "1. Create a database schema for storing feedback data\n2. Implement API endpoints for submitting feedback\n3. Develop analytics dashboards to visualize usage patterns\n4. Set up periodic reports on model performance\n5. Implement data export functionality for offline analysis\n\nDatabase schema example (PostgreSQL):\n```sql\n-- Queries table\nCREATE TABLE queries (\n    id SERIAL PRIMARY KEY,\n    query_text TEXT NOT NULL,\n    response_text TEXT NOT NULL,\n    request_id VARCHAR(36) NOT NULL,\n    timestamp TIMESTAMP NOT NULL DEFAULT NOW(),\n    user_id VARCHAR(255),\n    session_id VARCHAR(255),\n    response_time_ms INTEGER,\n    sources_used JSONB\n);\n\n-- Feedback table\nCREATE TABLE feedback (\n    id SERIAL PRIMARY KEY,\n    query_id INTEGER REFERENCES queries(id),\n    rating INTEGER CHECK (rating >= 1 AND rating <= 5),\n    is_helpful BOOLEAN,\n    comment TEXT,\n    timestamp TIMESTAMP NOT NULL DEFAULT NOW(),\n    user_id VARCHAR(255)\n);\n\n-- Analytics table for aggregated metrics\nCREATE TABLE daily_metrics (\n    id SERIAL PRIMARY KEY,\n    date DATE NOT NULL,\n    total_queries INTEGER NOT NULL,\n    avg_response_time_ms FLOAT,\n    positive_feedback_count INTEGER,\n    negative_feedback_count INTEGER,\n    unique_users INTEGER\n);\n```\n\nFeedback API endpoint:\n```python\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom pydantic import BaseModel\nimport psycopg2\nfrom psycopg2.extras import RealDictCursor\nimport os\n\napp = FastAPI()\n\n# Database connection\ndef get_db_connection():\n    conn = psycopg2.connect(\n        host=os.environ.get(\"DB_HOST\"),\n        database=os.environ.get(\"DB_NAME\"),\n        user=os.environ.get(\"DB_USER\"),\n        password=os.environ.get(\"DB_PASSWORD\")\n    )\n    conn.cursor_factory = RealDictCursor\n    return conn\n\nclass FeedbackRequest(BaseModel):\n    request_id: str\n    is_helpful: bool\n    rating: int = None\n    comment: str = None\n    user_id: str = None\n\n@app.post(\"/feedback\")\nasync def submit_feedback(feedback: FeedbackRequest):\n    conn = get_db_connection()\n    try:\n        cursor = conn.cursor()\n        \n        # First get the query_id from the request_id\n        cursor.execute(\n            \"SELECT id FROM queries WHERE request_id = %s\",\n            (feedback.request_id,)\n        )\n        result = cursor.fetchone()\n        \n        if not result:\n            raise HTTPException(status_code=404, detail=\"Query not found\")\n            \n        query_id = result[\"id\"]\n        \n        # Insert feedback\n        cursor.execute(\n            \"\"\"INSERT INTO feedback \n               (query_id, is_helpful, rating, comment, user_id) \n               VALUES (%s, %s, %s, %s, %s)\n            \"\"\",\n            (query_id, feedback.is_helpful, feedback.rating, \n             feedback.comment, feedback.user_id)\n        )\n        \n        conn.commit()\n        return {\"status\": \"success\"}\n    except Exception as e:\n        conn.rollback()\n        raise HTTPException(status_code=500, detail=str(e))\n    finally:\n        conn.close()\n```\n\nAnalytics dashboard (using Plotly Dash):\n```python\nimport dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nimport plotly.express as px\nimport pandas as pd\nimport psycopg2\nimport os\n\n# Database connection function\ndef get_data():\n    conn = psycopg2.connect(\n        host=os.environ.get(\"DB_HOST\"),\n        database=os.environ.get(\"DB_NAME\"),\n        user=os.environ.get(\"DB_USER\"),\n        password=os.environ.get(\"DB_PASSWORD\")\n    )\n    \n    # Query for daily metrics\n    daily_df = pd.read_sql(\"\"\"\n        SELECT date, total_queries, avg_response_time_ms, \n               positive_feedback_count, negative_feedback_count\n        FROM daily_metrics\n        ORDER BY date\n    \"\"\", conn)\n    \n    # Query for top questions\n    top_questions_df = pd.read_sql(\"\"\"\n        SELECT query_text, COUNT(*) as query_count\n        FROM queries\n        GROUP BY query_text\n        ORDER BY query_count DESC\n        LIMIT 10\n    \"\"\", conn)\n    \n    conn.close()\n    return daily_df, top_questions_df\n\n# Initialize Dash app\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    html.H1(\"SLM Analytics Dashboard\"),\n    \n    dcc.Tabs([\n        dcc.Tab(label=\"Usage Metrics\", children=[\n            html.Div([\n                html.H3(\"Daily Query Volume\"),\n                dcc.Graph(id=\"daily-volume-graph\"),\n                \n                html.H3(\"Response Time Trend\"),\n                dcc.Graph(id=\"response-time-graph\"),\n                \n                html.H3(\"Feedback Trend\"),\n                dcc.Graph(id=\"feedback-graph\"),\n            ])\n        ]),\n        \n        dcc.Tab(label=\"Top Questions\", children=[\n            html.Div([\n                html.H3(\"Most Frequent Queries\"),\n                dcc.Graph(id=\"top-questions-graph\"),\n            ])\n        ]),\n    ]),\n    \n    dcc.Interval(\n        id=\"interval-component\",\n        interval=60*60*1000,  # refresh every hour\n        n_intervals=0\n    )\n])\n\n@app.callback(\n    [Output(\"daily-volume-graph\", \"figure\"),\n     Output(\"response-time-graph\", \"figure\"),\n     Output(\"feedback-graph\", \"figure\"),\n     Output(\"top-questions-graph\", \"figure\")],\n    [Input(\"interval-component\", \"n_intervals\")]\n)\ndef update_graphs(n):\n    daily_df, top_questions_df = get_data()\n    \n    # Daily volume chart\n    volume_fig = px.line(\n        daily_df, x=\"date\", y=\"total_queries\",\n        title=\"Daily Query Volume\"\n    )\n    \n    # Response time chart\n    response_time_fig = px.line(\n        daily_df, x=\"date\", y=\"avg_response_time_ms\",\n        title=\"Average Response Time (ms)\"\n    )\n    \n    # Feedback chart\n    feedback_df = daily_df.copy()\n    feedback_df[\"positive_ratio\"] = feedback_df[\"positive_feedback_count\"] / \\\n                                  (feedback_df[\"positive_feedback_count\"] + \n                                   feedback_df[\"negative_feedback_count\"])\n    feedback_fig = px.line(\n        feedback_df, x=\"date\", y=\"positive_ratio\",\n        title=\"Positive Feedback Ratio\"\n    )\n    \n    # Top questions chart\n    top_questions_fig = px.bar(\n        top_questions_df, x=\"query_count\", y=\"query_text\",\n        orientation=\"h\", title=\"Top 10 Questions\"\n    )\n    \n    return volume_fig, response_time_fig, feedback_fig, top_questions_fig\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True)\n```",
        "testStrategy": "1. Test feedback submission with various input combinations\n2. Verify that feedback is correctly stored in the database\n3. Test analytics dashboard with sample data\n4. Validate that metrics are correctly calculated\n5. Test data export functionality\n6. Verify that the dashboard refreshes correctly\n7. Test with high volumes of feedback data to ensure performance\n8. Validate that user IDs are properly anonymized if required",
        "priority": "medium",
        "dependencies": [
          4,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Set Up Monitoring and Logging Infrastructure",
        "description": "Implement comprehensive monitoring and logging for both infrastructure and model performance to ensure reliability and detect anomalies.",
        "details": "1. Set up Prometheus and Grafana for metrics collection and visualization\n2. Configure alerts for system failures and performance anomalies\n3. Implement logging for API requests, model outputs, and system events\n4. Create dashboards for key metrics (GPU utilization, request rate, error rate, etc.)\n5. Set up model-specific monitoring (perplexity, output length distribution, etc.)\n\nPrometheus configuration example:\n```yaml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'slm_api'\n    static_configs:\n      - targets: ['api:8000']\n  \n  - job_name: 'node_exporter'\n    static_configs:\n      - targets: ['node_exporter:9100']\n      \n  - job_name: 'gpu_metrics'\n    static_configs:\n      - targets: ['gpu_exporter:9400']\n```\n\nAPI instrumentation with Prometheus client:\n```python\nfrom fastapi import FastAPI\nfrom prometheus_client import Counter, Histogram, start_http_server\nimport time\n\napp = FastAPI()\n\n# Define metrics\nREQUEST_COUNT = Counter(\n    'api_requests_total',\n    'Total count of requests by endpoint and status',\n    ['endpoint', 'status']\n)\n\nREQUEST_LATENCY = Histogram(\n    'api_request_duration_seconds',\n    'Request duration in seconds by endpoint',\n    ['endpoint']\n)\n\nMODEL_TOKEN_COUNT = Histogram(\n    'model_output_tokens',\n    'Distribution of output token counts',\n    ['endpoint']\n)\n\n# Start Prometheus metrics server\nstart_http_server(8000)\n\n# Middleware to track request metrics\n@app.middleware(\"http\")\nasync def metrics_middleware(request, call_next):\n    start_time = time.time()\n    \n    response = await call_next(request)\n    \n    duration = time.time() - start_time\n    endpoint = request.url.path\n    status = response.status_code\n    \n    REQUEST_COUNT.labels(endpoint=endpoint, status=status).inc()\n    REQUEST_LATENCY.labels(endpoint=endpoint).observe(duration)\n    \n    return response\n\n# Track token counts in generation endpoint\n@app.post(\"/generate\")\nasync def generate(request: GenerateRequest):\n    # ... existing code ...\n    \n    # Track token count\n    token_count = len(outputs[0])\n    MODEL_TOKEN_COUNT.labels(endpoint=\"/generate\").observe(token_count)\n    \n    # ... rest of function ...\n```\n\nGrafana dashboard configuration:\n```json\n{\n  \"dashboard\": {\n    \"id\": null,\n    \"title\": \"SLM Monitoring Dashboard\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"type\": \"graph\",\n        \"datasource\": \"Prometheus\",\n        \"targets\": [\n          {\n            \"expr\": \"sum(rate(api_requests_total[5m])) by (endpoint)\",\n            \"legendFormat\": \"{{endpoint}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Error Rate\",\n        \"type\": \"graph\",\n        \"datasource\": \"Prometheus\",\n        \"targets\": [\n          {\n            \"expr\": \"sum(rate(api_requests_total{status=~'5..'}[5m])) by (endpoint) / sum(rate(api_requests_total[5m])) by (endpoint)\",\n            \"legendFormat\": \"{{endpoint}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Response Time\",\n        \"type\": \"graph\",\n        \"datasource\": \"Prometheus\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, sum(rate(api_request_duration_seconds_bucket[5m])) by (endpoint, le))\",\n            \"legendFormat\": \"p95 {{endpoint}}\"\n          },\n          {\n            \"expr\": \"histogram_quantile(0.50, sum(rate(api_request_duration_seconds_bucket[5m])) by (endpoint, le))\",\n            \"legendFormat\": \"p50 {{endpoint}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"GPU Utilization\",\n        \"type\": \"graph\",\n        \"datasource\": \"Prometheus\",\n        \"targets\": [\n          {\n            \"expr\": \"nvidia_gpu_utilization\",\n            \"legendFormat\": \"GPU {{index}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"GPU Memory Usage\",\n        \"type\": \"graph\",\n        \"datasource\": \"Prometheus\",\n        \"targets\": [\n          {\n            \"expr\": \"nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes * 100\",\n            \"legendFormat\": \"GPU {{index}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Token Count Distribution\",\n        \"type\": \"heatmap\",\n        \"datasource\": \"Prometheus\",\n        \"targets\": [\n          {\n            \"expr\": \"sum(rate(model_output_tokens_bucket[5m])) by (le)\"\n          }\n        ]\n      }\n    ],\n    \"refresh\": \"10s\"\n  }\n}\n```\n\nAlert configuration:\n```yaml\ngroups:\n- name: slm_alerts\n  rules:\n  - alert: HighErrorRate\n    expr: sum(rate(api_requests_total{status=~\"5..\"}[5m])) / sum(rate(api_requests_total[5m])) > 0.05\n    for: 5m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"High error rate detected\"\n      description: \"Error rate is above 5% for the last 5 minutes\"\n      \n  - alert: SlowResponseTime\n    expr: histogram_quantile(0.95, sum(rate(api_request_duration_seconds_bucket[5m])) by (le)) > 2\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Slow response times detected\"\n      description: \"95th percentile response time is above 2 seconds\"\n      \n  - alert: HighGPUUtilization\n    expr: nvidia_gpu_utilization > 90\n    for: 15m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"High GPU utilization\"\n      description: \"GPU utilization is above 90% for 15 minutes\"\n      \n  - alert: LowDiskSpace\n    expr: node_filesystem_avail_bytes{mountpoint=\"/data\"} / node_filesystem_size_bytes{mountpoint=\"/data\"} * 100 < 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Low disk space\"\n      description: \"Less than 10% disk space available on /data\"\n```",
        "testStrategy": "1. Verify that metrics are correctly collected and displayed in Grafana\n2. Test alert triggering by simulating error conditions\n3. Validate that logs contain all necessary information\n4. Test dashboard functionality under load\n5. Verify that model-specific metrics are correctly calculated\n6. Test alert notifications (email, Slack, etc.)\n7. Validate that metrics are retained for the required period\n8. Test the system's ability to detect anomalies in model behavior",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement CI/CD Pipeline",
        "description": "Set up continuous integration and deployment pipelines for automated testing, model training, and deployment.",
        "details": "1. Set up a version-controlled repository for all code\n2. Configure CI pipelines for unit tests and data validation\n3. Implement CD for model training and deployment\n4. Create automated testing with synthetic prompts\n5. Set up staging and production environments\n\nGitHub Actions workflow example:\n```yaml\nname: SLM CI/CD Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.9'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install pytest pytest-cov\n        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n    - name: Run tests\n      run: |\n        pytest --cov=./ --cov-report=xml\n    - name: Upload coverage report\n      uses: codecov/codecov-action@v1\n\n  data_validation:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.9'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install great-expectations pandas\n        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n    - name: Validate data\n      run: |\n        python scripts/validate_data.py\n\n  build_and_push:\n    needs: [test, data_validation]\n    runs-on: ubuntu-latest\n    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Docker Buildx\n      uses: docker/setup-buildx-action@v1\n    - name: Login to DockerHub\n      uses: docker/login-action@v1\n      with:\n        username: ${{ secrets.DOCKERHUB_USERNAME }}\n        password: ${{ secrets.DOCKERHUB_TOKEN }}\n    - name: Build and push API image\n      uses: docker/build-push-action@v2\n      with:\n        context: ./api\n        push: true\n        tags: myorg/slm-api:latest,myorg/slm-api:${{ github.sha }}\n\n  deploy_staging:\n    needs: build_and_push\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Deploy to staging\n      uses: appleboy/ssh-action@master\n      with:\n        host: ${{ secrets.STAGING_HOST }}\n        username: ${{ secrets.STAGING_USERNAME }}\n        key: ${{ secrets.STAGING_SSH_KEY }}\n        script: |\n          cd /opt/slm\n          docker-compose pull\n          docker-compose up -d\n    - name: Run synthetic tests\n      run: |\n        python scripts/synthetic_tests.py --endpoint https://staging-api.example.com\n\n  deploy_production:\n    needs: deploy_staging\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Deploy to production\n      uses: appleboy/ssh-action@master\n      with:\n        host: ${{ secrets.PRODUCTION_HOST }}\n        username: ${{ secrets.PRODUCTION_USERNAME }}\n        key: ${{ secrets.PRODUCTION_SSH_KEY }}\n        script: |\n          cd /opt/slm\n          docker-compose pull\n          docker-compose up -d\n```\n\nSynthetic testing script:\n```python\nimport argparse\nimport requests\nimport json\nimport sys\n\ndef run_synthetic_tests(endpoint):\n    # Test cases with expected outputs or patterns\n    test_cases = [\n        {\n            \"query\": \"What is the main purpose of the SLM product?\",\n            \"expected_pattern\": \"domain-specific|knowledge|customized AI\"\n        },\n        {\n            \"query\": \"How does the retrieval system work?\",\n            \"expected_pattern\": \"vector|embedding|search|index\"\n        },\n        {\n            \"query\": \"This should be out of domain\",\n            \"expected_pattern\": \"don't know|cannot answer|insufficient information\"\n        }\n    ]\n    \n    results = []\n    for i, test in enumerate(test_cases):\n        print(f\"Running test {i+1}/{len(test_cases)}...\")\n        try:\n            response = requests.post(\n                f\"{endpoint}/generate\",\n                json={\"text\": test[\"query\"], \"temperature\": 0.1},\n                headers={\"X-API-Key\": \"test-key\"}\n            )\n            \n            if response.status_code != 200:\n                results.append({\n                    \"test\": i+1,\n                    \"status\": \"FAIL\",\n                    \"reason\": f\"API returned status code {response.status_code}\"\n                })\n                continue\n                \n            data = response.json()\n            model_response = data[\"response\"]\n            \n            # Check if response matches expected pattern\n            import re\n            if re.search(test[\"expected_pattern\"], model_response, re.IGNORECASE):\n                results.append({\n                    \"test\": i+1,\n                    \"status\": \"PASS\",\n                    \"response\": model_response[:100] + \"...\"\n                })\n            else:\n                results.append({\n                    \"test\": i+1,\n                    \"status\": \"FAIL\",\n                    \"reason\": \"Response did not match expected pattern\",\n                    \"response\": model_response[:100] + \"...\",\n                    \"expected_pattern\": test[\"expected_pattern\"]\n                })\n                \n        except Exception as e:\n            results.append({\n                \"test\": i+1,\n                \"status\": \"ERROR\",\n                \"reason\": str(e)\n            })\n    \n    # Print summary\n    passed = sum(1 for r in results if r[\"status\"] == \"PASS\")\n    print(f\"\\nTest Summary: {passed}/{len(test_cases)} tests passed\")\n    \n    for result in results:\n        print(f\"Test {result['test']}: {result['status']}\")\n        if result[\"status\"] != \"PASS\":\n            print(f\"  Reason: {result.get('reason', 'Unknown')}\")\n    \n    # Exit with error if any tests failed\n    if passed < len(test_cases):\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run synthetic tests against SLM API\")\n    parser.add_argument(\"--endpoint\", required=True, help=\"API endpoint URL\")\n    args = parser.parse_args()\n    \n    run_synthetic_tests(args.endpoint)\n```",
        "testStrategy": "1. Verify that CI pipeline correctly identifies code issues\n2. Test data validation with both valid and invalid data\n3. Verify that Docker images build correctly\n4. Test deployment to staging environment\n5. Validate synthetic testing with known good and bad responses\n6. Test rollback procedures\n7. Verify that production deployment works correctly\n8. Test the entire pipeline end-to-end with a simple code change",
        "priority": "medium",
        "dependencies": [
          3,
          4,
          9
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-17T10:58:08.291Z",
      "updated": "2025-06-17T11:54:52.699Z",
      "description": "Tasks for master context"
    }
  }
}